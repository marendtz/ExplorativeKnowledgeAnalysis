{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Implementation: Attention Layer and Transformer Block with Multi-Head Attention\n",
    "\n",
    "* see: https://peterbloem.nl/blog/transformers \n",
    "* and https://github.com/pbloem/former/blob/b438731ceeaf6c468f8b961bb07c2adde3b54a9f/former/modules.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test dataset to analyse model components\n",
    "\n",
    "# define the dimensions\n",
    "b = 1  # minibatch size\n",
    "t = 5  # sequence length (number of input vectors for each batch)\n",
    "k = 6  # vector dimension\n",
    "\n",
    "# create random input tensor\n",
    "X = torch.rand(b, t, k)\n",
    "\n",
    "# print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SelfAttention class (sequence to sequence operation)\n",
    "\n",
    "inherits from nn.Module (=base class for all neural network modules):\n",
    "* enables module to keep track of trainable parameters\n",
    "* ability to apply forward pass (def forward needs to be defined)\n",
    "\n",
    "forward pass: \n",
    "* input: sequence of t vectors of dimension k and a minibatch dimension b = tensor of size (b,t,k)\n",
    "* output: out_unified, dot, keys, queries, values (each as tensor of size (b,t,k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_(matrices, maskval=0.0, mask_diagonal=True):\n",
    "    \"\"\"\n",
    "    Masks out all values in the given batch of matrices where i <= j holds,\n",
    "    i < j if mask_diagonal is false\n",
    "\n",
    "    In place operation\n",
    "\n",
    "    :param tns:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    b, h, w = matrices.size()\n",
    "\n",
    "    indices = torch.triu_indices(h, w, offset=0 if mask_diagonal else 1)\n",
    "    matrices[:, indices[0], indices[1]] = maskval\n",
    "    \n",
    "def contains_nan(tensor):\n",
    "    return bool((tensor != tensor).sum() > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, k, heads=4, mask=True):\n",
    "\n",
    "        \"\"\"\n",
    "        :param k (int): input vector dimension (e.g. embedding dimension), which must be divisible by heads (h)\n",
    "        :param heads (int): number of heads, which will be used to split each input vector into h parts\n",
    "        :param mask (True): whether to apply masking to the softmax operation. necessary for generation models to prevent the model from seeing the future tokens.\n",
    "        \"\"\"        \n",
    "      \n",
    "        super().__init__()\n",
    "    \n",
    "        assert k % heads == 0\n",
    "        \n",
    "        self.k, self.heads, self.mask = k, heads, mask\n",
    "        \n",
    "        # setup the queries, keys and values for all heads\n",
    "        # nn.Linear(k, k, bias=False) creates a linear transformation with k inputs and k outputs, and no bias term.\n",
    "        self.tokeys    = nn.Linear(k, k, bias=False)\n",
    "        self.toqueries = nn.Linear(k, k, bias=False)\n",
    "        self.tovalues  = nn.Linear(k, k, bias=False)\n",
    "\n",
    "        # setup linear transformation after the multi-head self-attention operation in order to resize the output back to k dimensions\n",
    "        self.unifyheads = nn.Linear(k, k)\n",
    "    \n",
    "    \n",
    "    def forward(self, x, fix_attn_out_unified=None):\n",
    "        \"\"\"\n",
    "        :param x (torch.tensor): input tensor of shape (b, t, k) where b is the batch size, t is the sequence length and k is the input vector dimension\n",
    "        :param fix_attn_out_unified (torch.tensor): tensor of shape (b, t, k) which will be used to fix the output of the attention layer within causal mediation analysis\n",
    "        \"\"\"\n",
    "\n",
    "        b, t, k = x.size()\n",
    "        h = self.heads\n",
    "\n",
    "        assert k == self.k, f'Input embedding dim ({k}) should match model embedding dim ({self.k})'\n",
    "\n",
    "        # compute queries, keys and values for all heads, based on the weight matrices for each of them\n",
    "        queries = self.toqueries(x)\n",
    "        keys    = self.tokeys(x)   \n",
    "        values  = self.tovalues(x)\n",
    "        \n",
    "        # print(\"Example queries raw:\")\n",
    "        # print(queries)\n",
    "        # print(\"Example queries raw size:\")\n",
    "        # print(queries.size())\n",
    "\n",
    "        # split keys, queries and values into parts for each head\n",
    "        s = k // h\n",
    "\n",
    "        keys    = keys.view(b, t, h, s)\n",
    "        queries = queries.view(b, t, h, s)\n",
    "        values  = values.view(b, t, h, s)\n",
    "\n",
    "        # print(\"Example queries low dim (b, t, h, s):\")\n",
    "        # print(queries)\n",
    "        # print(\"Example queries low dim size (b, t, h, s):\")\n",
    "        # print(queries.size())\n",
    "\n",
    "        # fold heads into the batch dimension, to ensure, that multiplication is as easy as possible, since dot product needs to be created over all batches and heads anyway\n",
    "        # - transpose swaps second and third dimension\n",
    "        # - continous returns continous tensor, which means that the data is layed out continously in memory\n",
    "        \n",
    "        keys = keys.transpose(1, 2).contiguous().view(b * h, t, s)\n",
    "        queries = queries.transpose(1, 2).contiguous().view(b * h, t, s)\n",
    "        values = values.transpose(1, 2).contiguous().view(b * h, t, s)\n",
    "\n",
    "        # compute the dot product of queries and keys - also known as the attention map\n",
    "        dot = torch.bmm(queries, keys.transpose(1, 2))\n",
    "\n",
    "        # print(\"Dot queries and keys_T (must have t*t dimension to account for all necessary weights concerning number of input vectors):\")\n",
    "        # print(dot)\n",
    "        # print(\"Dot queries and keys_T size (b*h, t, t):\")\n",
    "        # print(dot.size())\n",
    "\n",
    "        # scale dot product, to avoid very large numbers as a result\n",
    "        dot = dot / math.sqrt(k)\n",
    "        assert dot.size() == (b*h, t, t), f'Matrix has size {dot.size()}, expected {(b*h, t, t)}.'\n",
    "\n",
    "        # print(\"Dot queries and keys_T scaled:\")\n",
    "        # print(dot)\n",
    "        # print(\"Dot queries and keys_T scaled size:\")\n",
    "        # print(dot.size())\n",
    "\n",
    "        # mask out the lower half of the dot matrix,including the diagonal (e.g. for text generation the model should not be able to see the future words)\n",
    "        if self.mask: \n",
    "            mask_(dot, maskval=float('-inf'), mask_diagonal=False)\n",
    "        \n",
    "        # normalize into [0,1] - dot now has row-wise self-attention probabilities over each low dimensional input s\n",
    "        dot = F.softmax(dot, dim=2) \n",
    "        \n",
    "        # print(\"dot queries and keys_T softmax:\")\n",
    "        # print(dot)\n",
    "        # print(\"dot queries and keys_T softmax size:\")\n",
    "        # print(dot.size())\n",
    "\n",
    "\n",
    "        # apply the self attention to the values and reshape tensor back\n",
    "        out = torch.bmm(dot, values)\n",
    "        out = out.view(b, h, t, s)\n",
    "        # print(\"out:\")\n",
    "        # print(out)\n",
    "        # print(\"out size:\")\n",
    "        # print(out.size())\n",
    "\n",
    "        # swap h, t back, unify heads\n",
    "        out = out.transpose(1, 2).contiguous()\n",
    "        out = out.view(b, t, s * h)\n",
    "        # print(\"out reshaped:\")\n",
    "        # print(out)\n",
    "        # print(\"out reshaped size:\")\n",
    "        # print(out.size())\n",
    "        \n",
    "        # compute the unified output of the multi-head self-attention operation\n",
    "        attn_out_unified = self.unifyheads(out)\n",
    "        # print(\"out unified:\")\n",
    "        # print(out_unified)\n",
    "        # print(\"out unified size:\")\n",
    "        # print(out_unified.size())\n",
    "\n",
    "        \n",
    "        # fix parts of the attention layer output within causal mediation analysis\n",
    "        if fix_attn_out_unified is not None:\n",
    "            mask = fix_attn_out_unified != 0\n",
    "            # print(\"mask:\")\n",
    "            # print(mask.shape)\n",
    "            # print(\"masked out unified:\")\n",
    "            # print(attn_out_unified.shape)\n",
    "            attn_out_unified[mask] = fix_attn_out_unified[mask]\n",
    "\n",
    "\n",
    "        # dot has size of (h, t, t)\n",
    "        return attn_out_unified, dot, keys, queries, values\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test SelfAttention class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare log directory for graph of SelfAttention model\n",
    "tensorboard_writer_sa = SummaryWriter(\"torchlogs/selfattention/\")\n",
    "selfattention = SelfAttention(k=6, heads=3, mask=True)\n",
    "# write model to tensorboard logs\n",
    "tensorboard_writer_sa.add_graph(selfattention, X)\n",
    "tensorboard_writer_sa.close()\n",
    "# print output\n",
    "attn_out_unified, dot, keys, queries, values = selfattention.forward(X)\n",
    "# print(attn_out_unified)\n",
    "\n",
    "# print output on adapted forward pass \n",
    "fix_attn_out_unified = torch.zeros(1, 5, 6)\n",
    "fix_attn_out_unified[0, 0, 0] = 1\n",
    "attn_out_unified, dot, keys, queries, values = selfattention.forward(X, fix_attn_out_unified=fix_attn_out_unified)\n",
    "# print(attn_out_unified)\n",
    "\n",
    "# to open tensorboard, run in terminal: tensorboard --logdir=./src/model-basic/torchlogs/selfattention/ --port=6006 and open browser at localhost:6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransformerBlock class (sequence to sequence operation)\n",
    "\n",
    "inherits from nn.Module (=base class for all neural network modules):\n",
    "* enables module to keep track of trainable parameters\n",
    "* ability to apply forward pass (def forward needs to be defined)\n",
    "\n",
    "forward pass: \n",
    "* input: sequence of t vectors of dimension k and a minibatch dimension b = tensor of size (b,t,k)\n",
    "* output: sequence of t vectors of dimension k and a minibatch dimension b = tensor of size (b,t,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "  def __init__(self, k, heads, mask=False, ff_hidden_mult=2, dropout=0.05):\n",
    "    \"\"\"\n",
    "    :param k (int): input vector dimension (e.g. embedding dimension)\n",
    "    :param heads (int): number of heads\n",
    "    :param mask (False, First): whether to apply masking to the softmax operation. Necessary for generation models to prevent the model from seeing the future tokens\n",
    "    :param ff_hidden_mult (int): the hidden multiplier in the feedforward neural network, common choice is 4.\n",
    "    :param dropout (float): the dropout rate\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "\n",
    "    self.attention = SelfAttention(k, heads=heads, mask=mask)\n",
    "    self.mask = mask\n",
    "\n",
    "    # setup layer normalization over k dimensions for each input vector separately (subtract mean and divide through standard diviation)\n",
    "    # nn-LayerNorm(k) expects input of size k in last dimension of tensor\n",
    "    self.norm1 = nn.LayerNorm(k)\n",
    "    self.norm2 = nn.LayerNorm(k)\n",
    "\n",
    "    # setup the feedforward neural network, which is applied to each position separately and identically\n",
    "    self.ff = nn.Sequential(\n",
    "      nn.Linear(k, ff_hidden_mult * k),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(ff_hidden_mult * k, k))\n",
    "\n",
    "    # setup dropout\n",
    "    self.do1 = nn.Dropout(dropout)\n",
    "    self.do2 = nn.Dropout(dropout)\n",
    "  \n",
    "  def forward(self, x, fix_attn_out_unified=None, fix_ff_out=None):\n",
    "    \"\"\"\n",
    "    :param x (torch.tensor): input tensor\n",
    "    :param fix_attn_out_unified (torch.tensor): tensor to fix parts of the attention layer output within causal mediation analysis\n",
    "    :param fix_ff_out (torch.tensor): tensor to fix parts of the feedforward layer output within causal mediation analysis\n",
    "    \"\"\"\n",
    "    # perform attention mechanism\n",
    "    attn_out_unified, dot, keys, queries, values = self.attention(x, fix_attn_out_unified=fix_attn_out_unified)\n",
    "\n",
    "    # add the residual connection and performs layer normalization\n",
    "    x = self.norm1(attn_out_unified + x) # reason why second linear layer in sequential is not followed by activation function\n",
    "    # apply dropout\n",
    "    x = self.do1(x)\n",
    "    # apply feedforward neural network\n",
    "    ff_out  = self.ff(x)\n",
    "    \n",
    "    # fix parts of the feedforward layer output within causal mediation analysis\n",
    "    if fix_ff_out is not None:\n",
    "      mask = fix_ff_out != 0\n",
    "      # print(\"mask:\")\n",
    "      # print(mask.shape)\n",
    "      # print(\"masked ff_out:\")\n",
    "      # print(ff_out.shape)\n",
    "      ff_out[mask] = fix_ff_out[mask]\n",
    "            \n",
    "    # add the residual connection and performs layer normalization\n",
    "    x = self.norm2(ff_out  + x)\n",
    "    # apply dropout\n",
    "    tb_out = self.do2(x)\n",
    "    \n",
    "    return tb_out, attn_out_unified, ff_out "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test TransformerBlock class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare log directory for graph of TransformerBlock model\n",
    "tensorboard_writer_tb = SummaryWriter(\"torchlogs/transformerblock/\")\n",
    "transformerblock = TransformerBlock(k=6, heads=3, mask=True, ff_hidden_mult=2, dropout=0.05)\n",
    "# write model to tensorboard logs\n",
    "tensorboard_writer_tb.add_graph(transformerblock, X)\n",
    "tensorboard_writer_tb.close()\n",
    "# print output\n",
    "tb_out, attn_out_unified, ff_out = transformerblock.forward(X)\n",
    "# print(ff_out)\n",
    "\n",
    "# print output on adapted forward pass \n",
    "fix_attn_out_unified = torch.zeros(1, 5, 6)\n",
    "fix_attn_out_unified[0, 0, 0] = 1\n",
    "fix_ff_out = torch.zeros(1, 5, 6)\n",
    "fix_ff_out[0, 0, 0] = 1\n",
    "tb_out, attn_out_unified, ff_out = transformerblock.forward(X, fix_attn_out_unified=fix_attn_out_unified, fix_ff_out=fix_ff_out)\n",
    "# print(ff_out)\n",
    "\n",
    "# to open tensorboard, run in terminal: tensorboard --logdir=./src/model-basic/torchlogs/transformerblock/ --port=6006 and open browser at localhost:6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GTransformer class (sequence to sequence operation)\n",
    "\n",
    "inherits from nn.Module (=base class for all neural network modules):\n",
    "* enables module to keep track of trainable parameters\n",
    "* ability to apply forward pass (def forward needs to be defined)\n",
    "\n",
    "forward pass: \n",
    "* input: sequence of t vectors of dimension k and a minibatch dimension b  = tensor of size (b,t,k)\n",
    "* output: log_probabilities over vocab for each position in the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d(tensor=None):\n",
    "    \"\"\"\n",
    "    Returns a device string either for the best available device,\n",
    "    or for the device corresponding to the argument\n",
    "    :param tensor:\n",
    "    \"\"\"\n",
    "    if tensor is None:\n",
    "        return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    return 'cuda' if tensor.is_cuda else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer for generating sequence output based on sequence input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k, heads, depth, t, num_tokens):\n",
    "        \"\"\"\n",
    "        :param k: Embedding dimension\n",
    "        :param heads: Number of attention heads\n",
    "        :param depth: Number of transformer blocks\n",
    "        :param t: Sequence length of input\n",
    "        :param num_tokens: Number of tokens (usually words) in the vocabulary\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_tokens = num_tokens\n",
    "\n",
    "        # setup embedding layers, which convert input tokens to their corresponding vectors for tokens and positions\n",
    "        # nn.Embedding is a simple lookup table that stores embeddings of a fixed dictionary and size\n",
    "        self.token_embedding = nn.Embedding(embedding_dim=k, num_embeddings=num_tokens)\n",
    "        self.pos_embedding = nn.Embedding(embedding_dim=k, num_embeddings=t)\n",
    "\n",
    "        # setup linear transformation to unify the embeddings of the tokens themselves and their positional encodings\n",
    "        self.unify_embeddings = nn.Linear(2*k, k)\n",
    "\n",
    "        # setup the stack of transformer blocks\n",
    "        tblocks = []\n",
    "        for i in range(depth):\n",
    "            tblocks.append(\n",
    "                TransformerBlock(k=k, heads=heads, mask=True, ff_hidden_mult=2, dropout=0.00))\n",
    "\n",
    "        self.tblocks = nn.Sequential(*tblocks)\n",
    "\n",
    "        # setup linear transformation that converts output vector of dimension k to a vector of dimension num_tokens\n",
    "        self.toprobs = nn.Linear(k, num_tokens)\n",
    "\n",
    "    def forward(self, x, fix_attn_out_unified_all=None, fix_ff_out_all=None):\n",
    "        \"\"\"\n",
    "        :param x: tensor (b, t, k) where b is batch size, t is token sequence length, k is the embedding dimension\n",
    "        :param fix_attn_out_unified_all: list of tensors of shape (b, t, k) which will be used to fix the output of the attention layer within causal mediation analysis\n",
    "        :param fix_ff_out_all: list of tensors of shape (b, t, k) which will be used to fix the output of the feedforward layer within causal mediation analysis\n",
    "        \"\"\"\n",
    "\n",
    "        # apply the token and positional embeddings\n",
    "        tokens = self.token_embedding(x)\n",
    "        b, t, k = tokens.size()\n",
    "        positions = self.pos_embedding(torch.arange(t, device=d()))[None, :, :].expand(b, t, k)\n",
    "        \n",
    "        # unify the embeddings of the tokens themselves and their positional encodings\n",
    "        x = self.unify_embeddings(torch.cat((tokens, positions), dim=2).view(-1, 2*k)).view(b, t, k)\n",
    "\n",
    "        # run the input through the stack of transformer blocks, while allowing the passing of the fixed attention and feedforward layer outputs for causal mediation analysis\n",
    "        attn_out_unified_all = []\n",
    "        ff_out_all = []\n",
    "        for i, block in enumerate(self.tblocks):\n",
    "            fix_attn_out_unified = fix_attn_out_unified_all[i] if fix_attn_out_unified_all is not None else None\n",
    "            # print(\"Shape of fix_attn_out_unified\") if fix_attn_out_unified is not None else None\n",
    "            # print(fix_attn_out_unified.size()) if fix_attn_out_unified is not None else None\n",
    "            fix_ff_out = fix_ff_out_all[i] if fix_ff_out_all is not None else None\n",
    "            # print(\"Shape of fix_ff_out\") if fix_ff_out is not None else None\n",
    "            # print(fix_ff_out.size()) if fix_ff_out is not None else None\n",
    "            x, attn_out_unified, ff_out = block(x, fix_attn_out_unified=fix_attn_out_unified, fix_ff_out=fix_ff_out)   # x = tb_out\n",
    "            attn_out_unified_all.append(attn_out_unified)\n",
    "            ff_out_all.append(ff_out)\n",
    "\n",
    "        # map the output vector of dimension k to a vector of dimension num_tokens\n",
    "        logits = self.toprobs(x.view(b*t, k)).view(b, t, self.num_tokens)\n",
    "\n",
    "        # compute the log probabilities\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "        return log_probs, attn_out_unified_all, ff_out_all   \n",
    "    \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def get_respresentations(self, x):\n",
    "        \"\"\"\n",
    "        Function for extracting the attention matrices of the whole Transformer for a single batch.\n",
    "        Input arguments same as the forward pass.\n",
    "        \"\"\"\n",
    "\n",
    "        tokens = self.token_embedding(x)\n",
    "        b, t, k = tokens.size()\n",
    "\n",
    "        positions = self.pos_embedding(torch.arange(t, device=d()))[None, :, :].expand(b, t, k)\n",
    "        x = self.unify_embeddings(torch.cat((tokens, positions), dim=2).view(-1, 2*k)).view(b, t, k)\n",
    "\n",
    "        attention_maps = []\n",
    "        key_matrices = []\n",
    "        query_matrices = []\n",
    "        value_matrices = []\n",
    "        out_matrices = []\n",
    "        probs_matrices = []\n",
    "        for i, block in enumerate(self.tblocks):\n",
    "            # set the model to evaluation mode, so that dropout is not applied\n",
    "            block.eval()\n",
    "            # get the attention maps, key, query and value matrices for each attention layer\n",
    "            attn_out_unified, dot, keys, queries, values = block.attention(x)      \n",
    "            attention_maps.append(dot)\n",
    "            key_matrices.append(keys)\n",
    "            query_matrices.append(queries)\n",
    "            value_matrices.append(values)    \n",
    "\n",
    "            # get the output of the transformer block\n",
    "            x, attn_out_unified, ff_out = block(x) # x = tb_out\n",
    "            out_matrices.append(x)\n",
    "            logits = self.toprobs(x.view(b*t, k)).view(b, t, self.num_tokens)\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            probs_matrices.append(log_probs)\n",
    "            # sets the model back to training mode\n",
    "            block.train()\n",
    "        return out_matrices, probs_matrices, attention_maps, key_matrices, query_matrices, value_matrices "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
