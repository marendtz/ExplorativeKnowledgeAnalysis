{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb # using this to import the modules notebook\n",
    "import modules # importing the notebook\n",
    "import torch\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_tokens: the number of different tokens in the corpus\n",
    "# t: the length of the sequences as input to the model\n",
    "# depth: depth of the network (number of transformer blocks)\n",
    "# heads: number of attention heads in the multi-head attention mechanism\n",
    "# k: embedding dimension (needs to be a multiple of heads)\n",
    "\n",
    "k = 6 # x * heads\n",
    "num_tokens = 10 # integers from 0 to 9\n",
    "heads = 3\n",
    "depth = 2\n",
    "t = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained model\n",
    "model = modules.GTransformer(k=k, heads=heads, depth=depth, t=t, num_tokens=num_tokens)\n",
    "model.load_state_dict(torch.load('gtransformer.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set token\n",
    "tokens = np.arange(num_tokens)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define class\n",
    "input_class = \"class2\"\n",
    "save_fig_path = f\"./images/causal_mediation_{input_class}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load aligned data from dataset and get source tensors\n",
    "aligned_data = torch.load(f'data/data_{input_class}.pt').tensors[0]\n",
    "print(aligned_data.shape)\n",
    "print(aligned_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create misaligned data by setting the third token to the same as the second token\n",
    "misaligned_data = aligned_data.clone()\n",
    "for i in range(aligned_data.shape[0]):\n",
    "    misaligned_data[i][2] = misaligned_data[i][1]\n",
    "print(misaligned_data.shape)\n",
    "print(misaligned_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tuples , that consist of each an example of aligned and misaligned data\n",
    "data = [(aligned_data[i], misaligned_data[i]) for i in range(aligned_data.shape[0])]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_mediation_analysis(data_tuple, model, heads, depth, k, t):\n",
    "    # make sure the model is in evaluation mode\n",
    "    model.eval()\n",
    "    # setup dictionaries, that will hold the results of the mediation analysis\n",
    "    total_result_all = {}\n",
    "    total_result_class ={}\n",
    "    direct_result_all_attn = {}\n",
    "    direct_result_all_ff = {}\n",
    "    direct_result_class_attn = {}\n",
    "    direct_result_class_ff = {}\n",
    "    indirect_result_all_attn = {}\n",
    "    indirect_result_all_ff = {}\n",
    "    indirect_result_class_attn = {}\n",
    "    indirect_result_class_ff = {}\n",
    "    attn_mediators = []\n",
    "    ff_mediators = []\n",
    "    # run the model on the aligned and misaligned data and retrieve log_probs, attn_out_unified_all, ff_out_all (no update of model parameters)\n",
    "    with torch.no_grad():\n",
    "        log_probs_aligned, attn_out_unified_all_aligned, ff_out_all_aligned = model(data_tuple[0].unsqueeze(0))\n",
    "        total_result_all['probs_aligned'] = torch.exp(log_probs_aligned)\n",
    "        total_result_class['probs_aligned'] = torch.exp(log_probs_aligned)[0][-1]\n",
    "        log_probs_misaligned, attn_out_unified_all_misaligned, ff_out_all_misaligned = model(data_tuple[1].unsqueeze(0))\n",
    "        total_result_all['probs_misaligned'] = torch.exp(log_probs_misaligned)\n",
    "        total_result_class['probs_misaligned'] = torch.exp(log_probs_misaligned)[0][-1]\n",
    "\n",
    "    # iterate over all heads and layers and mask the attention output for each head\n",
    "    attn_out_mediator_db = {}\n",
    "    attn_out_mediator_db = {}\n",
    "    for l in range(depth):\n",
    "        for h in range(heads):\n",
    "            # prep the attention layer mediator interventions, by masking the full attn_out for each head and layer\n",
    "            attn_out_unified_all_aligned_masked = [np.zeros_like(item) for item in attn_out_unified_all_aligned]\n",
    "            attn_out_unified_all_misaligned_masked = [np.zeros_like(item) for item in attn_out_unified_all_misaligned]\n",
    "            # e.g. attn_out_unified_all_aligned[0] for the first layer has torch.Size([1, 4, 6]), where 4 is seq_length, 6 is k which is created by k//3 of each head\n",
    "            # this means e.g. for head 1, only [1, 4, 0:2] is used, for head 2, only [1, 4, 2:4] is used, for head 3, only [1, 4, 4:6] is used \n",
    "            attn_out_mediator_db[l, h , \"aligned-fix\"] = attn_out_unified_all_aligned_masked.copy()\n",
    "            attn_out_mediator_db[l, h , \"aligned-fix\"][l][0, :, h * (k//heads):(h+1) * (k//heads)] = attn_out_unified_all_aligned[l][0, :, h * (k//heads):(h+1) * (k//heads)].clone()\n",
    "            attn_out_mediator_db[l, h , \"misaligned-fix\"] = attn_out_unified_all_misaligned_masked.copy()\n",
    "            attn_out_mediator_db[l, h , \"misaligned-fix\"][l][0, :, h * (k//heads):(h+1) * (k//heads)] = attn_out_unified_all_misaligned[l][0, :, h * (k//heads):(h+1) * (k//heads)].clone()\n",
    "    # iterate over all the feed forward layers and mask the feed forward output for each neuron\n",
    "    ff_out_mediator_db = {}\n",
    "    for l in range(depth):\n",
    "        for s in range(t):\n",
    "            for dim in range (k):\n",
    "                # prep the feed forward layer mediator interventions, by masking the full ff_out for each head and layer\n",
    "                ff_out_all_aligned_masked = [np.zeros_like(item) for item in ff_out_all_aligned]\n",
    "                ff_out_all_misaligned_masked = [np.zeros_like(item) for item in ff_out_all_misaligned]\n",
    "                ff_out_mediator_db[l,s,dim, \"aligned-fix\"] = ff_out_all_aligned_masked.copy()\n",
    "                ff_out_mediator_db[l,s,dim, \"aligned-fix\"][l][0, s, dim] = ff_out_all_aligned[l][0, s, dim].clone()\n",
    "                ff_out_mediator_db[l,s,dim, \"misaligned-fix\"] = ff_out_all_misaligned_masked.copy()\n",
    "                ff_out_mediator_db[l,s,dim, \"misaligned-fix\"][l][0, s, dim] = ff_out_all_misaligned[l][0, s, dim].clone()\n",
    "    \n",
    "    # perform the model runs for direct and indirect effects\n",
    "    with torch.no_grad():\n",
    "        # for all attention heads as mediators\n",
    "        for h in range(heads):\n",
    "            for l in range(depth):\n",
    "                log_probs_direct, _, _ = model(data_tuple[1].unsqueeze(0), fix_attn_out_unified_all=torch.tensor(attn_out_mediator_db[l,h, \"aligned-fix\"]))\t\n",
    "                attn_out_mediator_db[l, h , \"probs_direct\"]= torch.exp(log_probs_direct)\n",
    "                log_probs_indirect, _, _ = model(data_tuple[0].unsqueeze(0), fix_attn_out_unified_all=torch.tensor(attn_out_mediator_db[l,h, \"misaligned-fix\"]))\n",
    "                attn_out_mediator_db[l, h , \"probs_indirect\"] =torch.exp(log_probs_indirect)\n",
    "        # for all feed forward layers as mediators\n",
    "        for l in range(depth):\n",
    "            for s in range(t):\n",
    "                for dim in range(k):\n",
    "                    log_probs_direct, _, _ = model(data_tuple[1].unsqueeze(0), fix_ff_out_all=torch.tensor(ff_out_mediator_db[l,s,dim, \"aligned-fix\"]))\n",
    "                    ff_out_mediator_db[l,s,dim, \"probs_direct\"]= torch.exp(log_probs_direct)\n",
    "                    log_probs_indirect, _, _ = model(data_tuple[0].unsqueeze(0), fix_ff_out_all=torch.tensor(ff_out_mediator_db[l,s,dim, \"misaligned-fix\"]))\n",
    "                    ff_out_mediator_db[l,s,dim, \"probs_indirect\"] = torch.exp(log_probs_indirect)\n",
    "                \n",
    "    # calculate the direct and indirect effects using the log probabilities\n",
    "    for h in range(heads):\n",
    "        for l in range(depth):\n",
    "            attn_mediators.append('layer_'+str(l)+'_head_'+str(h))\n",
    "            direct_result_all_attn['layer_'+str(l)+'_head_'+str(h)] = attn_out_mediator_db[l, h , \"probs_direct\"] - total_result_all['probs_aligned']\n",
    "            direct_result_class_attn['layer_'+str(l)+'_head_'+str(h)] = attn_out_mediator_db[l, h , \"probs_direct\"][0][-1] - total_result_class['probs_aligned']\n",
    "            indirect_result_all_attn['layer_'+str(l)+'_head_'+str(h)] = attn_out_mediator_db[l, h , \"probs_indirect\"] - total_result_all['probs_aligned']\n",
    "            indirect_result_class_attn['layer_'+str(l)+'_head_'+str(h)] = attn_out_mediator_db[l, h , \"probs_indirect\"][0][-1] - total_result_class['probs_aligned']\n",
    "    for l in range(depth):\n",
    "        for s in range(t):\n",
    "            for dim in range(k):\n",
    "                ff_mediators.append('layer_'+str(l)+'_seq_'+str(s)+'_dim_'+str(dim))\n",
    "                direct_result_all_ff['layer_'+str(l)+'_seq_'+str(s)+'_dim_'+str(dim)] = ff_out_mediator_db[l,s,dim, \"probs_direct\"] - total_result_all['probs_aligned']\n",
    "                direct_result_class_ff['layer_'+str(l)+'_seq_'+str(s)+'_dim_'+str(dim)] = ff_out_mediator_db[l,s,dim, \"probs_direct\"][0][-1] - total_result_class['probs_aligned']\n",
    "                indirect_result_all_ff['layer_'+str(l)+'_seq_'+str(s)+'_dim_'+str(dim)] = ff_out_mediator_db[l,s,dim, \"probs_indirect\"] - total_result_all['probs_aligned']\n",
    "                indirect_result_class_ff['layer_'+str(l)+'_seq_'+str(s)+'_dim_'+str(dim)] = ff_out_mediator_db[l,s,dim, \"probs_indirect\"][0][-1] - total_result_class['probs_aligned']\n",
    "    return total_result_all, total_result_class , direct_result_all_attn, direct_result_all_ff, direct_result_class_attn, direct_result_class_ff, indirect_result_all_attn, indirect_result_all_ff, indirect_result_class_attn, indirect_result_class_ff, attn_mediators, ff_mediators\n",
    "               \n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the function\n",
    "total_result_all, total_result_class , direct_result_all_attn, direct_result_all_ff, direct_result_class_attn, direct_result_class_ff, indirect_result_all_attn, indirect_result_all_ff, indirect_result_class_attn, indirect_result_class_ff, attn_mediators, ff_mediators = causal_mediation_analysis(data[0], model, heads, depth, k, t)\n",
    "print(f\"attention mediators - number (depth*head): {len(attn_mediators)}\")\n",
    "print(attn_mediators)\n",
    "print(f\"feed forward mediators - number (depth*k*t): {len(ff_mediators)}\")\n",
    "print(ff_mediators)\n",
    "print(\"check keys\")\n",
    "print(direct_result_all_attn.keys())\n",
    "print(direct_result_all_ff.keys())\n",
    "print(\"check values (class)\")\n",
    "print(direct_result_class_attn['layer_0_head_0'].shape)\n",
    "print(direct_result_class_ff['layer_0_seq_0_dim_0'].shape)\n",
    "print(\"check values (all)\")\n",
    "print(direct_result_all_attn['layer_0_head_0'].shape)\n",
    "print(direct_result_all_ff['layer_0_seq_0_dim_0'].shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through the whole data, get results\n",
    "data_total_result_all = []\n",
    "data_total_result_class = []\n",
    "data_total_result_class_ff = []\n",
    "data_direct_result_all_attn = []\n",
    "data_direct_result_all_ff = []\n",
    "data_direct_result_class_attn = []\n",
    "data_direct_result_class_ff = []\n",
    "data_indirect_result_all_attn = []\n",
    "data_indirect_result_all_ff = []\n",
    "data_indirect_result_class_attn = []\n",
    "data_indirect_result_class_ff = []\n",
    "for i in range(len(data)):\n",
    "    total_result_all, total_result_class , direct_result_all_attn, direct_result_all_ff, direct_result_class_attn, direct_result_class_ff, indirect_result_all_attn, indirect_result_all_ff, indirect_result_class_attn, indirect_result_class_ff, attn_mediators, ff_mediators = causal_mediation_analysis(data[i], model, heads, depth, k, t)\n",
    "    data_total_result_all.append(total_result_all)\n",
    "    data_total_result_class.append(total_result_class)\n",
    "    data_direct_result_all_attn.append(direct_result_all_attn)\n",
    "    data_direct_result_all_ff.append(direct_result_all_ff)\n",
    "    data_direct_result_class_attn.append(direct_result_class_attn)\n",
    "    data_direct_result_class_ff.append(direct_result_class_ff)\n",
    "    data_indirect_result_all_attn.append(indirect_result_all_attn)\n",
    "    data_indirect_result_all_ff.append(indirect_result_all_ff)\n",
    "    data_indirect_result_class_attn.append(indirect_result_class_attn)\n",
    "    data_indirect_result_class_ff.append(indirect_result_class_ff)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_plot_all(effect, title, data):\n",
    "    flattened_data = []\n",
    "    for seq_pos in range(effect[0].shape[1]):\n",
    "        for vocab_pos in range(effect[0].shape[2]):\n",
    "            for i, sample in enumerate(effect):\n",
    "                flattened_data.append({\n",
    "                    'Sample': f'aligned:{data[i][0]} - misaligned:{data[i][1]}',\n",
    "                    'Sequence Position': seq_pos,\n",
    "                    'Vocabulary': vocab_pos,\n",
    "                    'Effect Value': round(float(sample[0, seq_pos, vocab_pos].item()),3)\n",
    "                })    \n",
    "    df = pd.DataFrame(flattened_data)    \n",
    "    fig = px.box(df, x='Sequence Position', y='Effect Value', color='Vocabulary', title=title, points='all', hover_data=['Sample'])\n",
    "    fig.update_yaxes(range = [-1.0,1.0],tickvals=np.linspace(-1.0, 1.0, 11))\n",
    "    fig.update_layout(width=1500,height=650,  font=dict(size=18, color='black')) \n",
    "    #fig.show()\n",
    "    fig.write_image(save_fig_path + title + \".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_plot_class(effect, title, data):\n",
    "    flattened_data = []    \n",
    "    for vocab_pos in range(effect[0].shape[0]):\n",
    "        for i,sample in enumerate(effect):\n",
    "            flattened_data.append({\n",
    "                'Sample': f'aligned:{data[i][0]} - misaligned:{data[i][1]}',\n",
    "                'Sequence Position': t-1,\n",
    "                'Vocabulary': vocab_pos,\n",
    "                'Effect Value': round(float(sample[vocab_pos].item()),3)\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(flattened_data)    \n",
    "    fig = px.box(df, x='Sequence Position', y='Effect Value', color='Vocabulary', title=title, points='all', hover_data=['Sample'])\n",
    "    fig.update_yaxes(range = [-1.0,1.0],tickvals=np.linspace(-1.0, 1.0, 11))\n",
    "    fig.update_layout(width=1500,height=650, font=dict(size=18, color='black')) \n",
    "    #fig.show()\n",
    "    fig.write_image(save_fig_path + title + \".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_plot_class_overview(effect, mediators,  title, data):\n",
    "    flattened_data = [] \n",
    "    for i, sample in enumerate(effect):\n",
    "        for m in mediators:         \n",
    "            for vocab_pos in range(sample[m].shape[0]):            \n",
    "                flattened_data.append({\n",
    "                    'Sample': f'aligned:{data[i][0]} - misaligned:{data[i][1]}',\n",
    "                    'Mediator': m,\n",
    "                    'Vocabulary': vocab_pos,\n",
    "                    'Effect Value': round(float(sample[m][vocab_pos].item()),3)\n",
    "                })\n",
    "    df = pd.DataFrame(flattened_data)    \n",
    "    fig = px.box(df, x='Mediator', y='Effect Value', color='Vocabulary', title=title, points='all', hover_data=['Sample'])\n",
    "    fig.update_yaxes(range = [-1.0,1.0],tickvals=np.linspace(-1.0, 1.0, 11))\n",
    "    fig.update_xaxes(tickangle=60, tickmode='linear', dtick=1)\n",
    "    fig.update_layout(width=1500,height=650,  font=dict(size=18, color='black')) \n",
    "    #fig.show()\n",
    "    fig.write_image(save_fig_path + title + \".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_plot_total_result_all(effect, title, data):\n",
    "    flattened_data = [] \n",
    "    for i, sample in enumerate(effect):    \n",
    "        for seq_pos in range(sample['probs_misaligned'][0].shape[0]):          \n",
    "            for vocab_pos in range(sample['probs_misaligned'][0].shape[1]):           \n",
    "                flattened_data.append({\n",
    "                    'Sample': f'aligned:{data[i][0]} - misaligned:{data[i][1]}',\n",
    "                    'Sequence Position': seq_pos,\n",
    "                    'Vocabulary': vocab_pos,\n",
    "                    'Effect Value': round(float(sample['probs_misaligned'][0][seq_pos][vocab_pos].item())-float(sample['probs_aligned'][0][seq_pos][vocab_pos].item()),3)\n",
    "                })\n",
    "    df = pd.DataFrame(flattened_data)    \n",
    "    fig = px.box(df, x='Sequence Position', y='Effect Value', color='Vocabulary', title=title, points='all', hover_data=['Sample'])\n",
    "    fig.update_yaxes(range = [-1.0,1.0],tickvals=np.linspace(-1.0, 1.0, 11))\n",
    "    fig.update_layout(width=1500,height=650,  font=dict(size=18, color='black')) \n",
    "    #fig.show()\n",
    "    fig.write_image(save_fig_path + title + \".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_plot_total_result_class(effect, class_position, title, data):\n",
    "    flattened_data = [] \n",
    "    for i, sample in enumerate(effect):                 \n",
    "        for vocab_pos in range(sample['probs_misaligned'].shape[0]):           \n",
    "            flattened_data.append({\n",
    "                'Sample': f'aligned:{data[i][0]} - misaligned:{data[i][1]}',\n",
    "                'Sequence Position': class_position,\n",
    "                'Vocabulary': vocab_pos,\n",
    "                'Effect Value': round(float(sample['probs_misaligned'][vocab_pos].item())-float(sample['probs_aligned'][vocab_pos].item()),3)\n",
    "            })\n",
    "    df = pd.DataFrame(flattened_data)    \n",
    "    fig = px.box(df, x='Sequence Position', y='Effect Value', color='Vocabulary', title=title, points='all', hover_data=['Sample'])\n",
    "    fig.update_yaxes(range = [-1.0,1.0],tickvals=np.linspace(-1.0, 1.0, 11))\n",
    "    fig.update_layout(width=1500,height=650,  font=dict(size=18, color='black')) \n",
    "    #fig.show()\n",
    "    fig.write_image(save_fig_path + title + \".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Mediators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for analysis of all sequence positions\n",
    "direct_effects_all = []\n",
    "indirect_effects_all = []\n",
    "direct_effects_class = []\n",
    "indirect_effects_class = []\n",
    "\n",
    "\n",
    "# Iterate over all mediators\n",
    "for mediator in attn_mediators:\n",
    "    direct_effect_values_all = []\n",
    "    indirect_effect_values_all = []\n",
    "    direct_effect_values_class = []\n",
    "    indirect_effect_values_class = []\n",
    "    \n",
    "    # Iterate over all test data\n",
    "    for i in range(len(data)):\n",
    "        direct_effect_values_all.append(data_direct_result_all_attn[i][mediator])\n",
    "        indirect_effect_values_all.append(data_indirect_result_all_attn[i][mediator])\n",
    "        direct_effect_values_class.append(data_direct_result_class_attn[i][mediator])\n",
    "        indirect_effect_values_class.append(data_indirect_result_class_attn[i][mediator])\n",
    "    \n",
    "    # append the effect values to the respective lists\n",
    "    direct_effects_all.append(direct_effect_values_all)\n",
    "    indirect_effects_all.append(indirect_effect_values_all)\n",
    "    direct_effects_class.append(direct_effect_values_class)\n",
    "    indirect_effects_class.append(indirect_effect_values_class)\n",
    "\n",
    "# Plot the direct and indirect effects for all attn mediators over all sequence positions\n",
    "for m, mediator in enumerate(attn_mediators):\n",
    "    direct_effect_all = direct_effects_all[m]\n",
    "    indirect_effect_all = indirect_effects_all[m]\n",
    "\n",
    "    # Plot the direct effects for attn mediators over all sequence positions\n",
    "    create_plot_all(direct_effect_all, f'Direct Effects (change probability) caused by Attention Mediators (for all sequence positions): {mediator}', data)\n",
    "\n",
    "    # Plot the indirect effects for attn mediators over all sequence positions\n",
    "    create_plot_all(indirect_effect_all, f'Indirect Effects (change probability) caused by Attention Mediators (for all sequence positions): {mediator}', data)\n",
    "\n",
    "\n",
    "    # direct_effect_class = direct_effects_class[m]\n",
    "    # indirect_effect_class = indirect_effects_class[m]\n",
    "\n",
    "    # # Plot the direct effects for attn mediators over class sequence position\n",
    "    # create_plot_class(direct_effect_class, f'Direct Effects (change probability) of Attention Mediator (for class positions): {mediator}', data)\n",
    "\n",
    "    # # Plot the indirect effects for attn mediators over class sequence position\n",
    "    # create_plot_class(indirect_effect_class, f'Indirect Effects (change probability) of Attention Mediator (for class positions): {mediator}', data)\n",
    "\n",
    "\n",
    "\n",
    "# Plot the direct effects for all attn mediators for the class position at once\n",
    "create_plot_class_overview(data_direct_result_class_attn, attn_mediators, 'Direct Effects (change probability) caused by Attention Mediators (for class position)', data)\n",
    "\n",
    "# Plot the indirect effects for all attn mediators for the class position at once\n",
    "create_plot_class_overview(data_indirect_result_class_attn, attn_mediators, 'Indirect Effects (change probability) caused by Attention Mediators (for class position)', data)\n",
    "\n",
    "# Plot total effect for all sequence positions\n",
    "create_plot_total_result_all(data_total_result_all, 'Total Effects (change probability) for all sequence positions', data)\n",
    "\n",
    "# Plot total effect for all sequence positions\n",
    "create_plot_total_result_class(data_total_result_class, t,  'Total Effects (change probability) for class position', data)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward Mediators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for analysis of all sequence positions\n",
    "direct_effects_all = []\n",
    "indirect_effects_all = []\n",
    "direct_effects_class = []\n",
    "indirect_effects_class = []\n",
    "\n",
    "\n",
    "# Iterate over all mediators\n",
    "for mediator in ff_mediators:\n",
    "    direct_effect_values_all = []\n",
    "    indirect_effect_values_all = []\n",
    "    direct_effect_values_class = []\n",
    "    indirect_effect_values_class = []\n",
    "    \n",
    "    # Iterate over all test data\n",
    "    for i in range(len(data)):\n",
    "        direct_effect_values_all.append(data_direct_result_all_ff[i][mediator])\n",
    "        indirect_effect_values_all.append(data_indirect_result_all_ff[i][mediator])\n",
    "        direct_effect_values_class.append(data_direct_result_class_ff[i][mediator])\n",
    "        indirect_effect_values_class.append(data_indirect_result_class_ff[i][mediator])\n",
    "    \n",
    "    # append the effect values to the respective lists\n",
    "    direct_effects_all.append(direct_effect_values_all)\n",
    "    indirect_effects_all.append(indirect_effect_values_all)\n",
    "    direct_effects_class.append(direct_effect_values_class)\n",
    "    indirect_effects_class.append(indirect_effect_values_class)\n",
    "\n",
    "# Plot the direct and indirect effects for all attn mediators over all sequence positions\n",
    "for m, mediator in enumerate(ff_mediators):\n",
    "    direct_effect_all = direct_effects_all[m]\n",
    "    indirect_effect_all = indirect_effects_all[m]\n",
    "\n",
    "    # Plot the direct effects for attn mediators over all sequence positions\n",
    "    create_plot_all(direct_effect_all, f'Direct Effects (change probability) caused by Feed Forward Mediators (for all sequence positions): {mediator}', data)\n",
    "\n",
    "    # Plot the indirect effects for attn mediators over all sequence positions\n",
    "    create_plot_all(indirect_effect_all, f'Indirect Effects (change probability) caused by Feed Forward Mediators (for all sequence positions): {mediator}', data)\n",
    "\n",
    "\n",
    "    # direct_effect_class = direct_effects_class[m]\n",
    "    # indirect_effect_class = indirect_effects_class[m]\n",
    "\n",
    "    # # Plot the direct effects for attn mediators over class sequence position\n",
    "    # create_plot_class(direct_effect_class, f'Direct Effects (change probability) of Attention Mediator (for class positions): {mediator}', data)\n",
    "\n",
    "    # # Plot the indirect effects for attn mediators over class sequence position\n",
    "    # create_plot_class(indirect_effect_class, f'Indirect Effects (change probability) of Attention Mediator (for class positions): {mediator}', data)\n",
    "\n",
    "\n",
    "\n",
    "# Plot the direct effects for all attn mediators for the class position at once\n",
    "create_plot_class_overview(data_direct_result_class_ff, ff_mediators, 'Direct Effects (change probability) caused by Feed Forward Mediators (for class position)', data)\n",
    "\n",
    "# Plot the indirect effects for all attn mediators for the class position at once\n",
    "create_plot_class_overview(data_indirect_result_class_ff, ff_mediators, 'Indirect Effects (change probability) caused by Feed Forward Mediators (for class position)', data)\n",
    "\n",
    "# Plot total effect for all sequence positions\n",
    "create_plot_total_result_all(data_total_result_all, 'Total Effects (change probability) for all sequence positions', data)\n",
    "\n",
    "# Plot total effect for all sequence positions\n",
    "create_plot_total_result_class(data_total_result_class,t, 'Total Effects (change probability) for class position', data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
