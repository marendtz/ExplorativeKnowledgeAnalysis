{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, tqdm\n",
    "import numpy as np\n",
    "import import_ipynb # used to import the modules notebook\n",
    "import modules # impor the notebook\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_tokens: the number of different tokens in the corpus\n",
    "# t: the length of the sequences as input to the model\n",
    "# depth: depth of the network (number of transformer blocks)\n",
    "# heads: number of attention heads in the multi-head attention mechanism\n",
    "# k: embedding dimension (needs to be a multiple of heads)\n",
    "\n",
    "k = 6 # x * heads\n",
    "num_tokens = 10 # integers from 0 to 9\n",
    "heads = 3\n",
    "depth = 2\n",
    "t = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vocab as integers\n",
    "vocab = np.arange(num_tokens)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- vocabulary = 1, 2, 3, 4, 5, 6, 7, 8, 9, 0\n",
    "- 0,1,2 are used as class labels \n",
    "- sequence length = 5\n",
    "\n",
    "- sequence class 0:\n",
    "    - increasing sequence\n",
    "    - e.g. 3,4,5,6,7\n",
    "- sequence class 1:\n",
    "    - decresing  sequence\n",
    "    - e.g. 9,8,7,6,5\n",
    "- sequence class 2:\n",
    "    - pairwise sequence\n",
    "    - e.g. 3,5,3,5,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_vector_data_increasing(vocab, vector_dim):\n",
    "    questions = []\n",
    "    answers = []\n",
    "    \n",
    "    for i in range(len(vocab)):\n",
    "        sequence = np.roll(vocab, i)[:vector_dim]\n",
    "        questions.append(sequence)\n",
    "\n",
    "        sequence_roll = np.roll(sequence, -1)\n",
    "        sequence_roll[-1] = 0\n",
    "        answers.append(sequence_roll)\n",
    "            \n",
    "    questions = np.array(questions)\n",
    "    answers = np.array(answers)\n",
    "    \n",
    "    return questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "questions, answers = generate_synthetic_vector_data_increasing(vocab[3:], t)\n",
    "print(\"There are {} questions and {} answers for the increasing style\".format(questions.shape[0], answers.shape[0]))\n",
    "for i,question in enumerate(questions):\n",
    "    print(str(question) + \" -> \" + str(answers[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_vector_data_decreasing(vocab, vector_dim):\n",
    "    questions = []\n",
    "    answers = []\n",
    "    \n",
    "    for i in range(len(vocab)):\n",
    "        reverse_vocab = np.flip(vocab)\n",
    "        sequence = np.roll(reverse_vocab, i)[:vector_dim]\n",
    "        questions.append(sequence)\n",
    "        \n",
    "        sequence_roll = np.roll(sequence, -1)\n",
    "        sequence_roll[-1] = 1\n",
    "        answers.append(sequence_roll)\n",
    "            \n",
    "    \n",
    "    questions = np.array(questions)\n",
    "    answers = np.array(answers)\n",
    "    \n",
    "    return questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "questions, answers = generate_synthetic_vector_data_decreasing(vocab[3:], t)\n",
    "print(\"There are {} questions and {} answers for the decreasing style\".format(questions.shape[0], answers.shape[0]))\n",
    "for i,question in enumerate(questions):\n",
    "    print(str(question) + \" -> \" + str(answers[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_vector_data_recurring(vocab, vector_dim):\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for first in vocab:        \n",
    "        for second in vocab:\n",
    "            if second == first:\n",
    "                continue\n",
    "            sequence = [first, second] * math.ceil(vector_dim/2)\n",
    "            questions.append(sequence[0:vector_dim])\n",
    "            sequence_roll = np.roll(sequence[0:vector_dim], -1)\n",
    "            sequence_roll[-1] = 2\n",
    "            answers.append(sequence_roll)\n",
    "        \n",
    "    questions = np.array(questions)\n",
    "    answers = np.array(answers)\n",
    "\n",
    "    return questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "questions, answers = generate_synthetic_vector_data_recurring(vocab[3:], t)\n",
    "print(\"There are {} questions and {} answers for the recurring style\".format(questions.shape[0], answers.shape[0]))\n",
    "print(\"Example: {} --> {}\".format(questions[0], answers[0]))\n",
    "print(\"Example: {} --> {}\".format(questions[1], answers[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data \n",
    "questions_class0, answers_class0 = generate_synthetic_vector_data_increasing(vocab[3:], t)\n",
    "questions_class1, answers_class1 = generate_synthetic_vector_data_decreasing(vocab[3:], t)\n",
    "questions_class2, answers_class2 = generate_synthetic_vector_data_recurring(vocab[3:], t)\n",
    "\n",
    "# # create train and test sets for each class\n",
    "# train_test_split = 0.8\n",
    "# train_test_split_index = int(train_test_split * len(questions_class0))\n",
    "# train_questions_class0 = questions_class0[:train_test_split_index]\n",
    "# train_answers_class0 = answers_class0[:train_test_split_index]\n",
    "# test_questions_class0 = questions_class0[train_test_split_index:]\n",
    "# test_answers_class0 = answers_class0[train_test_split_index:]\n",
    "\n",
    "# train_test_split_index = int(train_test_split * len(questions_class1))\n",
    "# train_questions_class1 = questions_class1[:train_test_split_index]\n",
    "# train_answers_class1 = answers_class1[:train_test_split_index]\n",
    "# test_questions_class1 = questions_class1[train_test_split_index:]\n",
    "# test_answers_class1 = answers_class1[train_test_split_index:]\n",
    "\n",
    "# train_test_split_index = int(train_test_split * len(questions_class2))\n",
    "# train_questions_class2 = questions_class2[:train_test_split_index]\n",
    "# train_answers_class2 = answers_class2[:train_test_split_index]\n",
    "# test_questions_class2 = questions_class2[train_test_split_index:]\n",
    "# test_answers_class2 = answers_class2[train_test_split_index:]\n",
    "\n",
    "\n",
    "\n",
    "# # create tensor datasets for each class\n",
    "# data_train_class0 = TensorDataset(torch.tensor(train_questions_class0), torch.tensor(train_answers_class0))\n",
    "# data_test_class0 = TensorDataset(torch.tensor(test_questions_class0), torch.tensor(test_answers_class0))\n",
    "# data_train_class1 = TensorDataset(torch.tensor(train_questions_class1), torch.tensor(train_answers_class1))\n",
    "# data_test_class1 = TensorDataset(torch.tensor(test_questions_class1), torch.tensor(test_answers_class1))\n",
    "# data_train_class2 = TensorDataset(torch.tensor(train_questions_class2), torch.tensor(train_answers_class2))\n",
    "# data_test_class2 = TensorDataset(torch.tensor(test_questions_class2), torch.tensor(test_answers_class2))\n",
    "\n",
    "# create full datasets without train and test split\n",
    "data_class0 = TensorDataset(torch.tensor(questions_class0), torch.tensor(answers_class0))\n",
    "data_class1 = TensorDataset(torch.tensor(questions_class1), torch.tensor(answers_class1))\n",
    "data_class2 = TensorDataset(torch.tensor(questions_class2), torch.tensor(answers_class2))\n",
    "\n",
    "\n",
    "# # save tensor datasets to ./data\n",
    "# torch.save(data_train_class0, './data/data_train_class0.pt')\n",
    "# torch.save(data_test_class0, './data/data_test_class0.pt')\n",
    "# torch.save(data_train_class1, './data/data_train_class1.pt')\n",
    "# torch.save(data_test_class1, './data/data_test_class1.pt')\n",
    "# torch.save(data_train_class2, './data/data_train_class2.pt')\n",
    "# torch.save(data_test_class2, './data/data_test_class2.pt')\n",
    "\n",
    "# save full datasets to ./data\n",
    "torch.save(data_class0, './data/data_class0.pt')\n",
    "torch.save(data_class1, './data/data_class1.pt')\n",
    "torch.save(data_class2, './data/data_class2.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print specs of train and test data\n",
    "# print(\"Train data class 0: \", len(data_train_class0))\n",
    "# print(\"Test data class 0: \", len(data_test_class0))\n",
    "# print(\"example: \", data_train_class0[0])\n",
    "\n",
    "# print(\"Train data class 1: \", len(data_train_class1))\n",
    "# print(\"Test data class 1: \", len(data_test_class1))\n",
    "# print(\"example: \", data_train_class1[0])\n",
    "\n",
    "# print(\"Train data class 2: \", len(data_train_class2))\n",
    "# print(\"Test data class 2: \", len(data_test_class2))\n",
    "# print(\"example: \", data_train_class2[0])\n",
    "\n",
    "# print specs of full data\n",
    "print(\"Full data class 0: \", len(data_class0))\n",
    "print(\"example: \", data_class0[0])\n",
    "\n",
    "print(\"Full data class 1: \", len(data_class1))\n",
    "print(\"example: \", data_class1[0])\n",
    "\n",
    "print(\"Full data class 2: \", len(data_class2))\n",
    "print(\"example: \", data_class2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hasattr(modules, 'GTransformer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = modules.GTransformer(k=k, heads=heads, depth=depth, t=t, num_tokens=num_tokens)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "num_batches = 10000\n",
    "batch_size = 9  # batch size needs to be a multiple of the class number\n",
    "lr=0.001\n",
    "gradient_clipping = 1.0\n",
    "test_every = 100\n",
    "test_batchsize = 9\n",
    "patience = 5\n",
    "\n",
    "\n",
    "# tensorboard writer\n",
    "tensorboard_writer_gt = SummaryWriter(\"torchlogs/gtransformer/\")\n",
    "\n",
    "# define optimization for learning\n",
    "opt = torch.optim.Adam(lr=lr, params=model.parameters())\n",
    "\n",
    "# define the learning rate scheduler\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=1000, gamma=0.1)\n",
    "\n",
    "for i in tqdm(range(num_batches)):\n",
    "    # learning rate warmup\n",
    "    opt.zero_grad()\n",
    "    # sample source and target from the three classes for batch \n",
    "    number_data = batch_size // 3\n",
    "    # source_class0, target_class0 = data_train_class0[torch.randint(0, len(data_train_class0), (number_data,))]\n",
    "    # source_class1, target_class1 = data_train_class1[torch.randint(0, len(data_train_class1), (number_data,))]\n",
    "    # source_class2, target_class2 = data_train_class2[torch.randint(0, len(data_train_class2), (number_data,))]\n",
    "    source_class0, target_class0 = data_class0[torch.randint(0, len(data_class0), (number_data,))]\n",
    "    source_class1, target_class1 = data_class1[torch.randint(0, len(data_class1), (number_data,))]\n",
    "    source_class2, target_class2 = data_class2[torch.randint(0, len(data_class2), (number_data,))]\n",
    "    # concatenate the sources and targets\n",
    "    source = torch.cat([source_class0, source_class1, source_class2])\n",
    "    target = torch.cat([target_class0, target_class1, target_class2])\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        source, target = source.cuda(), target.cuda() \n",
    "\n",
    "    output, _, _ = model(source)\n",
    "    # print(\"output: \", output)\n",
    "    # print(\"output\", output.shape)\n",
    "\n",
    "    loss = F.nll_loss(output.transpose(2, 1), target, reduction='mean')\n",
    "    tensorboard_writer_gt.add_scalar('gtransformer/train-loss', float(loss.item()), i * batch_size)\n",
    "    loss.backward()\n",
    "\n",
    "    # clip gradients: if the total gradient vector has a length > 1, we clip it back down to 1.\n",
    "    if gradient_clipping > 0.0:\n",
    "       nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping)\n",
    "\n",
    "    # update the model weights and the learning rate\n",
    "    opt.step()\n",
    "    # scheduler.step()\n",
    "\n",
    "    # track the best model\n",
    "    best_eval_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    eval_no_improve = 0\n",
    "\n",
    "    \n",
    "    if i != 0 and (i % test_every == 0 or i == num_batches - 1):\n",
    "        upto = data_class0.tensors[0].size(0) if i == num_batches - 1 else test_batchsize // 3\n",
    "        # sample source and target from the three classes for batch \n",
    "        # source_class0, target_class0 = data_test_class0[torch.randint(0, len(data_test_class0), (upto,))]\n",
    "        # source_class1, target_class1 = data_test_class1[torch.randint(0, len(data_test_class1), (upto,))]\n",
    "        # source_class2, target_class2 = data_test_class2[torch.randint(0, len(data_test_class2), (upto,))]\n",
    "        source_class0, target_class0 = data_class0[torch.randint(0, len(data_class0), (upto,))]\n",
    "        source_class1, target_class1 = data_class1[torch.randint(0, len(data_class1), (upto,))]\n",
    "        source_class2, target_class2 = data_class2[torch.randint(0, len(data_class2), (upto,))]\n",
    "        # concatenate the sources and targets\n",
    "        source = torch.cat([source_class0, source_class1, source_class2])\n",
    "        target = torch.cat([target_class0, target_class1, target_class2])\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                source, target = source.cuda(), target.cuda()\n",
    "\n",
    "            output, _ , _ = model(source)\n",
    "            eval_loss = F.nll_loss(output.transpose(2, 1), target, reduction='mean')\n",
    "            tensorboard_writer_gt.add_scalar('gtransformer/eval-loss', float(eval_loss.item()), i * batch_size)\n",
    "            print(f'epoch{i}: {loss:.4} loss')\n",
    "\n",
    "            if eval_loss < best_eval_loss:\n",
    "                best_eval_loss = eval_loss\n",
    "                best_model_state = model.state_dict()  \n",
    "                torch.save(best_model_state, 'best_model.pth')  \n",
    "                eval_no_improve = 0\n",
    "            else:\n",
    "                eval_no_improve += 1\n",
    "\n",
    "            if eval_no_improve >= patience:\n",
    "                print(f'Early stopping at epoch {i}')\n",
    "                break\n",
    "\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model state after training\n",
    "model.load_state_dict(torch.load('best_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start TensorBoard (or use the command line: tensorboard --logdir=./src/model-basic/torchlogs/gtransformer/)\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./torchlogs/gtransformer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model and load it\n",
    "torch.save(model.state_dict(), 'gtransformer.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
