{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Inner Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import import_ipynb # using this to import the modules notebook\n",
    "import modules # importing the notebook\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_tokens: the number of different tokens in the corpus\n",
    "# t: the length of the sequences as input to the model\n",
    "# depth: depth of the network (number of transformer blocks)\n",
    "# heads: number of attention heads in the multi-head attention mechanism\n",
    "# k: embedding dimension (needs to be a multiple of heads)\n",
    "\n",
    "k = 6 # x * heads\n",
    "num_tokens = 10 # integers from 0 to 9\n",
    "heads = 3\n",
    "depth = 2\n",
    "t = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained model\n",
    "model = modules.GTransformer(k=k, heads=heads, depth=depth, t=t, num_tokens=num_tokens)\n",
    "model.load_state_dict(torch.load('gtransformer.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set token\n",
    "tokens = np.arange(num_tokens)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze Attention Maps + Key, Value, Query Matrices for example input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define example input for class 0: increasing \n",
    "input = torch.tensor([[4,5,5,5,4]], dtype=torch.long)\n",
    "save_fig_path = \"./images/misaligned_class2/\"\n",
    "\n",
    "\n",
    "print(\"input:\", input)\n",
    "print(\"input size\", input[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate output for the input sequence, without adapting the weights + set model to eval mode\n",
    "model.eval() \n",
    "with torch.no_grad():\n",
    "    output, _, _ = model(input)\n",
    "    # print(\"output\", output)\n",
    "    print(\"output (vocab distribution for each output dim)\", output.shape)\n",
    "    # get most propable token from log_prob output: \n",
    "    print(\"output\", torch.argmax(torch.exp(output), dim=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get attention maps, key matrices, queries matrices, and values matrices\n",
    "out_matrices, probs_matrices, attention_maps, key_matrices, query_matrices, value_matrices  = model.get_respresentations(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze attention maps\n",
    "print(f\"number of attention maps (concat over all heads in each layer): {len(attention_maps)} - heads used: {heads}, attention layers used: {depth}\")\n",
    "attention_map = attention_maps[0]\n",
    "print(\"Shape of each attention map: \", attention_map.shape) # beware that each head has its own dot product matrix representating the attention\n",
    "assert attention_map.shape[0] == heads \n",
    "assert attention_map.shape[1] == attention_map.shape[2] == t # matrice needs to be k*k\n",
    "print(\"Number of heads: \", attention_map.shape[0])\n",
    "print(\"Sequence length: \", attention_map.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for only one batch (one sequence that went through the 2 TransformerBlocks (depth = 2) that are integrated in the GTransformer)\n",
    "batch = 0\n",
    "\n",
    "nlayers = len(attention_maps)\n",
    "nheads = attention_maps[0].shape[0]\n",
    "t = attention_maps[0].shape[1]\n",
    "\n",
    "\n",
    "print(f\"Number of displayable layers: {nlayers}\")\n",
    "print(f\"Number of displayable heads: {nheads}\")\n",
    "\n",
    "cols = nheads\n",
    "rows = nlayers\n",
    "\n",
    "fig, ax = plt.subplots(rows, cols, figsize=(15,15))\n",
    "\n",
    "for row in range(rows):\n",
    "    attention_map = attention_maps[row]\n",
    "    print(attention_map.shape)\n",
    "    for col in range(cols):   \n",
    "        data = attention_map[col,:,:].detach().cpu()\n",
    "        vmin = data.min().item()\n",
    "        vmax = data.max().item()\n",
    "        ax[row, col].imshow(data, origin=\"lower\",vmin = vmin, vmax =vmax)\n",
    "        ax[row, col].set_xticks(list(range(t)))\n",
    "        ax[row, col].set_xticklabels((f\"In-Position {i}\" for i,token in enumerate(input[0].tolist())), rotation=90)\n",
    "        ax[row, col].set_yticks(list(range(t)))\n",
    "        ax[row, col].set_yticklabels(f\"Out-Position {i}\" for i,token in enumerate(input[0].tolist()))\n",
    "        ax[row, col].set_title(f\"TF block {row}, Head {col}\")\n",
    "        for i in range(data.shape[0]):\n",
    "            for j in range(data.shape[1]):\n",
    "                text = ax[row, col].text(j, i, f\"{data[i, j]:.2f}\", ha=\"center\", va=\"center\", color=\"w\")\n",
    "        \n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Attention Maps\", y=1.02)\n",
    "plt.savefig(save_fig_path+ \"attention_maps.png\")\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze key, query amd value matrices\n",
    "print(f\"number of key matrices (concat over all heads in each layer): {len(key_matrices)}- heads used: {heads}, attention layers used: {depth}\")\n",
    "key_matrice = key_matrices[0]\n",
    "print(\"Shape of each key matrix: \", key_matrice.shape) # beware that each head has its own key matrice\n",
    "# each head specific matrice needs to have the dimension of sequence in one dimension and k/heads in the other\n",
    "assert key_matrice.shape[0] == heads\n",
    "assert key_matrice.shape[1] == t\n",
    "assert key_matrice.shape[2] == k // heads\n",
    "print(\"Number of heads: \", key_matrice.shape[0])\n",
    "print(\"Sequence length: \", key_matrice.shape[1])\n",
    "print(\"k // heads: \", key_matrice.shape[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for only one batch (one sequence that went through the 2 TransformerBlocks (depth = 2) that are integrated in the GTransformer)\n",
    "batch = 0\n",
    "\n",
    "nlayers = len(key_matrices)\n",
    "nheads = key_matrices[0].shape[0]\n",
    "seq_len =key_matrices[0].shape[1]\n",
    "\n",
    "print(f\"Number of displayable layers: {nlayers}\")\n",
    "print(f\"Number of displayable heads: {nheads}\")\n",
    "\n",
    "cols = nheads\n",
    "rows = nlayers\n",
    "\n",
    "fig, ax = plt.subplots(rows, cols, figsize=(15,15))\n",
    "\n",
    "for row in range(rows):\n",
    "    key_matrice = key_matrices[row]\n",
    "    for col in range(cols):   \n",
    "        data =key_matrice[col,:,:].detach().cpu()\n",
    "        vmin = data.min().item()\n",
    "        vmax = data.max().item()\n",
    "        ax[row, col].imshow(data, origin=\"lower\",vmin = vmin, vmax =vmax)\n",
    "        ax[row, col].set_xticks(list(range(k // heads)))\n",
    "        ax[row, col].set_yticks(list(range(seq_len)))\n",
    "        ax[row, col].set_yticklabels(f\"Position: {i}\" for i in range(0,len(input[0].tolist())))\n",
    "        ax[row, col].set_title(f\"TF block {row}, Head {col}\")\n",
    "        for i in range(data.shape[0]):\n",
    "            for j in range(data.shape[1]):\n",
    "                text = ax[row, col].text(j, i, f\"{data[i, j]:.2f}\", ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "plt.suptitle(\"Key Matrices (Keys of dimension k//heads for each token)\", y=1.02)\n",
    "plt.savefig(save_fig_path+ \"key_matrices.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze query matrices\n",
    "print(f\"number of query matrices (concat over all heads in each layer): {len(query_matrices)}- heads used: {heads}, attention layers used: {depth}\")\n",
    "query_matrice = query_matrices[0]\n",
    "print(\"Shape of each query matrix: \", query_matrice.shape) # beware that each head has its own query matrice\n",
    "# each head specific matrice needs to have the dimension of sequence in one dimension and k/heads in the other\n",
    "assert query_matrice.shape[0] == heads\n",
    "assert query_matrice.shape[1] == t\n",
    "assert query_matrice.shape[2] == k // heads\n",
    "print(\"Number of heads: \", key_matrice.shape[0])\n",
    "print(\"Sequence length: \", key_matrice.shape[1])\n",
    "print(\"k // heads: \", key_matrice.shape[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for only one batch (one sequence that went through the 2 TransformerBlocks (depth = 2) that are integrated in the GTransformer)\n",
    "batch = 0\n",
    "\n",
    "nlayers = len(query_matrices)\n",
    "nheads = query_matrices[0].shape[0]\n",
    "t = query_matrices[0].shape[1]\n",
    "\n",
    "print(f\"Number of displayable layers: {nlayers}\")\n",
    "print(f\"Number of displayable heads: {nheads}\")\n",
    "\n",
    "cols = nheads\n",
    "rows = nlayers\n",
    "\n",
    "fig, ax = plt.subplots(rows, cols, figsize=(15,15))\n",
    "\n",
    "for row in range(rows):\n",
    "    query_matrice = query_matrices[row]\n",
    "    for col in range(cols):   \n",
    "        data = query_matrice[col,:,:].detach().cpu()\n",
    "        vmin = data.min().item()\n",
    "        vmax = data.max().item()\n",
    "        ax[row, col].imshow(data, origin=\"lower\",vmin = vmin, vmax =vmax)\n",
    "        ax[row, col].set_xticks(list(range(k // heads)))\n",
    "        ax[row, col].set_yticks(list(range(t)))\n",
    "        ax[row, col].set_yticklabels(f\"Position: {i}\" for i in range(0,len(input[0].tolist())))\n",
    "        ax[row, col].set_title(f\"TF block {row}, Head {col}\")\n",
    "        for i in range(data.shape[0]):\n",
    "            for j in range(data.shape[1]):\n",
    "                text = ax[row, col].text(j, i, f\"{data[i, j]:.2f}\", ha=\"center\", va=\"center\", color=\"w\")\n",
    "        \n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Query Matrices (Queries of dimension k//heads for each token)\", y=1.02)\n",
    "plt.savefig(save_fig_path+ \"query_matrices.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze key matrices\n",
    "print(f\"number of key matrices (concat over all heads in each layer): {len(key_matrices)}- heads used: {heads}, attention layers used: {depth}\")\n",
    "key_matrice = key_matrices[0]\n",
    "print(\"Shape of each key matrix: \", key_matrice.shape) # beware that each head has its own key matrice\n",
    "# each head specific matrice needs to have the dimension of sequence in one dimension and k/heads in the other\n",
    "assert key_matrice.shape[0] == heads\n",
    "assert key_matrice.shape[1] == t\n",
    "assert key_matrice.shape[2] == k // heads\n",
    "print(\"Number of heads: \", key_matrice.shape[0])\n",
    "print(\"Sequence length: \", key_matrice.shape[1])\n",
    "print(\"k // heads: \", key_matrice.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for only one batch (one sequence that went through the 2 TransformerBlocks (depth = 2) that are integrated in the GTransformer)\n",
    "batch = 0\n",
    "\n",
    "nlayers = len(key_matrices)\n",
    "nheads = key_matrices[0].shape[0]\n",
    "t = key_matrices[0].shape[1]\n",
    "\n",
    "\n",
    "print(f\"Number of displayable layers: {nlayers}\")\n",
    "print(f\"Number of displayable heads: {nheads}\")\n",
    "\n",
    "cols = nheads\n",
    "rows = nlayers\n",
    "\n",
    "fig, ax = plt.subplots(rows, cols, figsize=(15,15))\n",
    "\n",
    "for row in range(rows):\n",
    "    key_matrice = key_matrices[row]\n",
    "    print(key_matrice.shape)\n",
    "    for col in range(cols):   \n",
    "        data = key_matrice[col,:,:].detach().cpu()\n",
    "        vmin = data.min().item()\n",
    "        vmax = data.max().item()\n",
    "        ax[row, col].imshow(data, origin=\"lower\",vmin = vmin, vmax =vmax)\n",
    "        ax[row, col].set_xticks(list(range(k // heads)))\n",
    "        ax[row, col].set_yticks(list(range(t)))\n",
    "        ax[row, col].set_yticklabels(f\"Position: {i}\" for i in range(0,len(input[0].tolist())))\n",
    "        ax[row, col].set_title(f\"TF block {row}, Head {col}\")\n",
    "        for i in range(data.shape[0]):\n",
    "            for j in range(data.shape[1]):\n",
    "                text = ax[row, col].text(j, i, f\"{data[i, j]:.2f}\", ha=\"center\", va=\"center\", color=\"w\")\n",
    "        \n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Key Matrices (Values of dimension k//heads for each token)\", y=1.02)\n",
    "plt.savefig(save_fig_path+ \"value_matrices.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze out matrices\n",
    "print(f\"number of out matrices: {len(out_matrices)}\")\n",
    "out_matrice = out_matrices[0]\n",
    "print(\"Shape of each out matrix: \", out_matrice.shape) # beware that each head has its own out matrice\n",
    "assert out_matrice.shape[0] == 1\n",
    "assert out_matrice.shape[1] == t\n",
    "assert out_matrice.shape[2] == k \n",
    "print(\"Number of output: \", out_matrice.shape[0])\n",
    "print(\"Sequence length: \",out_matrice.shape[1])\n",
    "print(\"k : \",out_matrice.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for only one batch (one sequence that went through the 2 TransformerBlocks (depth = 2) that are integrated in the GTransformer)\n",
    "batch = 0\n",
    "\n",
    "nlayers = len(out_matrices)\n",
    "t = out_matrices[0].shape[1]\n",
    "k = out_matrices[0].shape[2]\n",
    "\n",
    "print(f\"Number of displayable layers: {nlayers}\")\n",
    "\n",
    "cols = 1\n",
    "rows = nlayers\n",
    "\n",
    "fig, ax = plt.subplots(rows, cols, figsize=(15,15))\n",
    "\n",
    "for row in range(rows):\n",
    "    out_matrice = out_matrices[row]\n",
    "    data = out_matrice.detach().cpu().squeeze(0)\n",
    "    vmin = data.min().item()\n",
    "    vmax = data.max().item()\n",
    "    ax[row].imshow(data, origin=\"lower\",vmin = vmin, vmax =vmax)\n",
    "    ax[row].set_xticks(list(range(k)))\n",
    "    ax[row].set_yticks(list(range(t)))\n",
    "    ax[row].set_yticklabels(f\"Position: {i}\" for i in range(0,len(input[0].tolist())))\n",
    "    ax[row].set_title(f\"TF block {row}\")\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            text = ax[row].text(j, i, f\"{data[i, j]:.2f}\", ha=\"center\", va=\"center\", color=\"w\")\n",
    "        \n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Out Matrices (Output Embeddings of dimension k for each output position)\", y=1.02)\n",
    "plt.savefig(save_fig_path+  \"out_matrices.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze prob matrices\n",
    "print(f\"number of prob matrices: {len(probs_matrices)}\")\n",
    "probs_matrice = probs_matrices[0]\n",
    "print(\"Shape of each prob matrix: \", probs_matrice.shape) # beware that each head has its own prob matrix\n",
    "assert probs_matrice.shape[0] == 1\n",
    "assert probs_matrice.shape[1] == t\n",
    "assert probs_matrice.shape[2] == len(tokens)\n",
    "print(\"Number of output: \", probs_matrice.shape[0])\n",
    "print(\"Sequence length: \", probs_matrice.shape[1])\n",
    "print(\"Number of tokens: \", probs_matrice.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for only one batch (one sequence that went through the 2 TransformerBlocks (depth = 2) that are integrated in the GTransformer)\n",
    "batch = 0\n",
    "\n",
    "nlayers = len(probs_matrices)\n",
    "t = probs_matrices[0].shape[1]\n",
    "num_tokens = probs_matrices[0].shape[2]\n",
    "\n",
    "print(f\"Number of displayable layers: {nlayers}\")\n",
    "\n",
    "cols = 1\n",
    "rows = nlayers\n",
    "\n",
    "fig, ax = plt.subplots(rows, cols, figsize=(15,15))\n",
    "\n",
    "for row in range(rows):\n",
    "    probs_matrice = probs_matrices[row]  \n",
    "    data = probs_matrice.detach().cpu().squeeze(0)\n",
    "    data = torch.exp(data)\n",
    "    # assert that sums to 1 \n",
    "    assert torch.allclose(data.sum(dim=1), torch.ones(t), atol=1e-6)\n",
    "    vmin = data.min().item()\n",
    "    vmax = data.max().item()\n",
    "    ax[row].imshow(data, origin=\"lower\",vmin = vmin, vmax =vmax)\n",
    "    ax[row].set_xticks(list(range(len(tokens))))\n",
    "    ax[row].set_xticklabels(f\"Vocab-Token: {i}\" for i in tokens)\n",
    "    ax[row].set_yticks(list(range(t)))\n",
    "    ax[row].set_yticklabels(f\"Position: {i}\" for i in range(0,len(input[0].tolist())))\n",
    "    ax[row].set_title(f\"TF block {row}\")\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            text = ax[row].text(j, i, f\"{data[i, j]:.3f}\", ha=\"center\", va=\"center\", color=\"w\")\n",
    "        \n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Probs Matrices (Token Log Probabilities for each output position)\", y=1.02)\n",
    "plt.savefig(save_fig_path+ \"prob_matrices.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
